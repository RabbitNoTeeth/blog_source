---
title: 张量运算
description: 张量运算
lang: zh-CN
---

深度神经网络学到的所有变换都可以简化为数值数据张量上的一些 **张量运算（tensor operation）**，例如加上张量、乘以张量等。

## 1. 逐元素运算

Keras 中 relu运算和加法都是逐元素（element-wise）的运算，即该运算独立地应用于张量中的每个元素，也就是说，这些运算非常适合大规模并行实现（向量化实现）。

如果你想对逐元素运算编写简单的 Python 实现，那么可以用 for 循环。下列代码是对逐元素 relu 运算的简单实现。

```python
def naive_relu(x):
    assert len(x.shape) == 2    // x 是一个Numpy的2D张量
    x = x.copy()                // 避免覆盖输入张量
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] = max(x[i, j], 0)
    return x
```

对于加法采用同样的实现方法。

```python
def naive_add(x, y):
    assert len(x.shape) == 2        // x 和 y 是Numpy的2D张量
    assert x.shape == y.shape
    x = x.copy()                    // 避免覆盖输入张量
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[i, j]
    return x
```

根据同样的方法，你可以实现逐元素的乘法、减法等。

在实践中处理 Numpy 数组时，这些运算都是优化好的 Numpy 内置函数，这些函数将大量运算交给安装好的基础线性代数子程序（BLAS， basic linear algebra subprograms）实现（没装
的话，应该装一个）。 BLAS 是低层次的、高度并行的、高效的张量操作程序，通常用 Fortran 或 C 语言来实现。

因此，在 Numpy 中可以直接进行下列逐元素运算，速度非常快。

```python
import numpy as np
z = x + y                   // 逐元素的相加
z = np.maximum(z, 0.)       // 逐元素的relu
```

## 2. 广播

如果将两个形状不同的张量相加，较小的张量会被广播（broadcast），以匹配较大张量的形状。

广播包含以下两步。

1. 向较小的张量添加轴（叫作**广播轴**），使其 ndim 与较大的张量相同。

2. 将较小的张量沿着新轴重复，使其形状与较大的张量相同。