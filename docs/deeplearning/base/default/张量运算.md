---
title: 张量运算
description: 张量运算
lang: zh-CN
---

深度神经网络学到的所有变换都可以简化为数值数据张量上的一些 **张量运算（tensor operation）**，例如加上张量、乘以张量等。

## 1. 逐元素运算

Keras 中 relu运算和加法都是逐元素（element-wise）的运算，即该运算独立地应用于张量中的每个元素，也就是说，这些运算非常适合大规模并行实现（向量化实现）。

如果你想对逐元素运算编写简单的 Python 实现，那么可以用 for 循环。下列代码是对逐元素 relu 运算的简单实现。

```python
def naive_relu(x):
    assert len(x.shape) == 2    // x 是一个Numpy的2D张量
    x = x.copy()                // 避免覆盖输入张量
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] = max(x[i, j], 0)
    return x
```

对于加法采用同样的实现方法。

```python
def naive_add(x, y):
    assert len(x.shape) == 2        // x 和 y 是Numpy的2D张量
    assert x.shape == y.shape
    x = x.copy()                    // 避免覆盖输入张量
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[i, j]
    return x
```

根据同样的方法，你可以实现逐元素的乘法、减法等。

在实践中处理 Numpy 数组时，这些运算都是优化好的 Numpy 内置函数，这些函数将大量运算交给安装好的基础线性代数子程序（BLAS， basic linear algebra subprograms）实现（没装
的话，应该装一个）。 BLAS 是低层次的、高度并行的、高效的张量操作程序，通常用 Fortran 或 C 语言来实现。

因此，在 Numpy 中可以直接进行下列逐元素运算，速度非常快。

```python
import numpy as np
z = x + y                   // 逐元素的相加
z = np.maximum(z, 0.)       // 逐元素的relu
```

## 2. 广播

如果将两个形状不同的张量相加，较小的张量会被广播（broadcast），以匹配较大张量的形状。

广播包含以下两步。

1. 向较小的张量添加轴（叫作**广播轴**），使其 ndim 与较大的张量相同。

2. 将较小的张量沿着新轴重复，使其形状与较大的张量相同。

来看一个具体的例子。假设 X 的形状是 (32, 10)， y 的形状是 (10,)。首先，我们给 y 添加空的第一个轴，这样 y 的形状变为 (1, 10)。然后，我们将 y 沿着新轴重复 32 次，这样
得到的张量 Y 的形状为 (32, 10)，并且 `Y[i, :] == y for i in range(0, 32)`。现在，我们可以将 X 和 Y 相加，因为它们的形状相同。

在实际的实现过程中并不会创建新的 2D 张量，因为那样做非常低效。重复的操作完全是虚拟的，它只出现在算法中，而没有发生在内存中。但想象将向量沿着新轴重复 10 次，是一种很有用的思维模型。下面是一种简单的实现。

```python
def naive_add_matrix_and_vector(x, y):
    assert len(x.shape) == 2            // x 是一个 Numpy 的 2D 张量
    assert len(y.shape) == 1            // y 是一个 Numpy 向量
    assert x.shape[1] == y.shape[0]
    x = x.copy()                        // 避免覆盖输入张量
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[j]
    return x
```

如果一个张量的形状是 (a, b, ... n, n+1, ... m)，另一个张量的形状是 (n, n+1, ... m)，那么你通常可以利用广播对它们做两个张量之间的逐元素运算。广播操作会自动应用于从 a 到 n-1 的轴。

下面这个例子利用广播将逐元素的 maximum 运算应用于两个形状不同的张量。

```python
import numpy as np
x = np.random.random((64, 3, 32, 10))   // x 是形状为 (64, 3, 32, 10) 的随机张量
y = np.random.random((32, 10))          // y 是形状为 (32, 10) 的随机张量
z = np.maximum(x, y)                    // 输出 z 的形状是 (64, 3, 32, 10)，与 x 相同
```

## 3. 点积

点积运算，也叫 **张量积**（tensor product，不要与逐元素的乘积弄混），是最常见也最有用的张量运算。与逐元素的运算不同，它将输入张量的元素合并在一起。

在 Numpy、 Keras、 Theano 和 TensorFlow 中，都是用 * 实现逐元素乘积。 TensorFlow 中的点积使用了不同的语法，但在 Numpy 和 Keras 中，都是用标准的 dot 运算符来实现点积。

```python
import numpy as np
z = np.dot(x, y)
```

数学符号中的点（.）表示点积运算。

```python
z=x.y
```

从数学的角度来看，点积运算做了什么？我们首先看一下两个向量 x 和 y 的点积。其计算过程如下。

```python
def naive_vector_dot(x, y):
    assert len(x.shape) == 1            // x 和 y 都是 Numpy 向量
    assert len(y.shape) == 1
    assert x.shape[0] == y.shape[0]
    z = 0.
    for i in range(x.shape[0]):
        z += x[i] * y[i]
    return z
```

注意，两个向量之间的点积是一个标量，而且只有元素个数相同的向量之间才能做点积。

你还可以对一个矩阵 x 和一个向量 y 做点积，返回值是一个向量，其中每个元素是 y 和 x 的每一行之间的点积。其实现过程如下。

```python
import numpy as np
def naive_matrix_vector_dot(x, y):
    assert len(x.shape) == 2                // x 是一个 Numpy 矩阵
    assert len(y.shape) == 1                // y 是一个 Numpy 向量
    assert x.shape[1] == y.shape[0]         // x 的第 1 维和 y 的第 0 维大小必须相同
    z = np.zeros(x.shape[0])                // 这个运算返回一个全是 0 的向量，其形状与 x.shape[0] 相同
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            z[i] += x[i, j] * y[j]
    return z
```

你还可以复用前面写过的代码，从中可以看出矩阵 - 向量点积与向量点积之间的关系。

```python
def naive_matrix_vector_dot(x, y):
    z = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        z[i] = naive_vector_dot(x[i, :], y)
    return z
```

注意，如果两个张量中有一个的 ndim 大于 1，那么 dot 运算就不再是对称的，也就是说，dot(x, y) 不等于 dot(y, x)。

当然，点积可以推广到具有任意个轴的张量。最常见的应用可能就是两个矩阵之间的点积。对于两个矩阵 x 和 y，当且仅当 x.shape[1] == y.shape[0] 时，你才可以对它们做点积（dot(x, y)）。得到的结果是一个形状为 (x.shape[0], y.shape[1]) 的矩阵，其元素为 x 的行与 y 的列之间的点积。其简单实现如下。

```python
def naive_matrix_dot(x, y):
    assert len(x.shape) == 2                        // x 和 y 都是Numpy矩阵
    assert len(y.shape) == 2
    assert x.shape[1] == y.shape[0]                 // x 的第 1 维和 y 的第 0 维大小必须相同
    z = np.zeros((x.shape[0], y.shape[1]))          // 这个运算返回特定形状的零矩阵
    for i in range(x.shape[0]):                     // 遍历 x 的所有行
        for j in range(y.shape[1]):                 // 然后遍历 y 的所有列
            row_x = x[i, :]
            column_y = y[:, j]
            z[i, j] = naive_vector_dot(row_x, column_y)
    return z
```

为了便于理解点积的形状匹配，可以将输入张量和输出张量像图 2-5 中那样排列，利用可视化来帮助理解。

<img src="/img/deeplearning/2-5.png" style="zoom:100%;" />

图 2-5 中， x、 y 和 z 都用矩形表示（元素按矩形排列）。 x 的行和 y 的列必须大小相同，因此 x 的宽度一定等于 y 的高度。如果你打算开发新的机器学习算法，可能经常要画这种图。

更一般地说，你可以对更高维的张量做点积，只要其形状匹配遵循与前面 2D 张量相同的原则：

(a, b, c, d) . (d,) -> (a, b, c)

(a, b, c, d) . (d, e) -> (a, b, c, e)

以此类推。

## 4. 变形

**张量变形（tensor reshaping）** 是指改变张量的行和列，以得到想要的形状。变形后的张量的元素总个数与初始张量相同。简单的例子可以帮助我们理解张量变形。

```python
>>> x = np.array([[0., 1.],
                  [2., 3.],
                  [4., 5.]])
>>> print(x.shape)
(3, 2)

>>> x = x.reshape((6, 1))
>>> x
array([[ 0.],
       [ 1.],
       [ 2.],
       [ 3.],
       [ 4.],
       [ 5.]])
       
>>> x = x.reshape((2, 3))
>>> x
array([[ 0., 1., 2.],
       [ 3., 4., 5.]])
```

经常遇到的一种特殊的张量变形是 **转置（transposition）**。对矩阵做转置是指将行和列互换，使 x\[i, :] 变为 x\[:, i]。

```python
>>> x = np.zeros((300, 20))
>>> x = np.transpose(x)
>>> print(x.shape)
(20, 300)
```

## 5. 张量运算的几何解释

对于张量运算所操作的张量，其元素可以被解释为某种几何空间内点的坐标，因此所有的张量运算都有几何解释。举个例子，我们来看加法。首先有这样一个向量：

A = \[0.5, 1]

它是二维空间中的一个点（见图 2-6）。常见的做法是将向量描绘成原点到这个点的箭头，如图 2-7 所示。

<img src="/img/deeplearning/2-6.png" style="zoom:100%;" />

假设又有一个点： B = \[1, 0.25]，将它与前面的 A 相加。从几何上来看，这相当于将两个向量箭头连在一起，得到的位置表示两个向量之和对应的向量（见图 2-8）。

<img src="/img/deeplearning/2-8.png" style="zoom:100%;" />

通常来说，仿射变换、旋转、缩放等基本的几何操作都可以表示为张量运算。举个例子，要将一个二维向量旋转 theta 角，可以通过与一个 2 × 2 矩阵做点积来实现，这个矩阵为 R = \[u, v]，其中 u 和 v 都是平面向量： u = \[cos(theta), sin(theta)]， v = \[-sin(theta), cos(theta)]。

## 6. 深度学习的几何解释

前面讲过，神经网络完全由一系列张量运算组成，而这些张量运算都只是输入数据的几何变换。因此，你可以将神经网络解释为高维空间中非常复杂的几何变换，这种变换可以通过许多简单的步骤来实现。

对于三维的情况，下面这个思维图像是很有用的。想象有两张彩纸：一张红色，一张蓝色。将其中一张纸放在另一张上。现在将两张纸一起揉成小球。这个皱巴巴的纸球就是你的输入数据，每张纸对应于分类问题中的一个类别。神经网络（或者任何机器学习模型）要做的就是找到可以让纸球恢复平整的变换，从而能够再次让两个类别明确可分。通过深度学习，这一过程可以用三维空间中一系列简单的变换来实现，比如你用手指对纸球做的变换，每次做一个动作，如图 2-9 所示。

<img src="/img/deeplearning/2-9.png" style="zoom:100%;" />

让纸球恢复平整就是机器学习的内容：为复杂的、高度折叠的数据流形找到简洁的表示。现在你应该能够很好地理解，为什么深度学习特别擅长这一点：它将复杂的几何变换逐步分解为一长串基本的几何变换，这与人类展开纸球所采取的策略大致相同。深度网络的每一层都通过变换使数据解开一点点——许多层堆叠在一起，可以实现非常复杂的解开过程。
